"""
Advanced ML Model Training - Standalone Version
"""

import os
import sys
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score
)
import joblib
import json
from datetime import datetime

class AdvancedModelTrainer:
    
    def __init__(self, dataset_path, model_type='ensemble'):
        self.dataset_path = dataset_path
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = [
            'token_similarity',
            'structural_similarity',
            'ast_similarity',
            'identical_segments_count',
            'near_identical_segments_count',
            'complexity_difference',
            'loc_difference',
            'function_count_ratio',
            'class_count_ratio',
            'comment_ratio_difference',
            'blank_lines_ratio',
            'avg_line_length_difference',
            'unique_token_ratio',
            'operator_count_ratio',
            'operand_count_ratio'
        ]
        self.best_params = None
    
    def load_data(self):
        """Load and preprocess training data"""
        print(f"ðŸ“‚ Loading data from {self.dataset_path}...")
        
        if not os.path.exists(self.dataset_path):
            raise FileNotFoundError(f"âŒ Dataset not found at {self.dataset_path}")
        
        df = pd.read_csv(self.dataset_path)
        print(f"âœ“ Loaded {len(df)} samples with {len(df.columns)} features")
        
        # Handle missing values
        df = df.fillna(0)
        
        # Separate features and labels
        X = df[self.feature_names].values
        y = df['is_similar'].values
        
        # Print class distribution
        unique, counts = np.unique(y, return_counts=True)
        print(f"\nðŸ“Š Class Distribution:")
        for label, count in zip(unique, counts):
            print(f"  Class {label}: {count} samples ({count/len(y)*100:.2f}%)")
        
        return X, y, df
    
    def create_model(self):
        """Create model based on type"""
        print(f"\nðŸ”§ Creating {self.model_type} model...")
        
        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1,
                verbose=1
            )
        
        elif self.model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=7,
                random_state=42,
                verbose=1
            )
        
        elif self.model_type == 'svm':
            self.model = SVC(
                kernel='rbf',
                C=10.0,
                gamma='scale',
                probability=True,
                random_state=42,
                verbose=True
            )
        
        elif self.model_type == 'neural_network':
            self.model = MLPClassifier(
                hidden_layer_sizes=(128, 64, 32),
                activation='relu',
                solver='adam',
                learning_rate='adaptive',
                max_iter=500,
                random_state=42,
                verbose=True
            )
        
        elif self.model_type == 'ensemble':
            print("Creating ensemble of Random Forest, Gradient Boosting, and SVM...")
            
            rf = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                random_state=42,
                n_jobs=-1
            )
            
            gb = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=7,
                random_state=42
            )
            
            svm = SVC(
                kernel='rbf',
                C=10,
                gamma='scale',
                probability=True,
                random_state=42
            )
            
            self.model = VotingClassifier(
                estimators=[('rf', rf), ('gb', gb), ('svm', svm)],
                voting='soft',
                n_jobs=-1
            )
        
        else:
            raise ValueError(f"Unknown model type: {self.model_type}")
        
        return self.model
    
    def train(self, X_train, y_train):
        """Train the model"""
        print("\nðŸŽ“ Training model...")
        print(f"   Training samples: {len(X_train)}")
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # Train model
        self.model.fit(X_train_scaled, y_train)
        print("âœ“ Training completed!")
    
    def evaluate(self, X_test, y_test):
        """Evaluate model performance"""
        print("\nðŸ“Š Evaluating model performance...")
        
        # Scale test features
        X_test_scaled = self.scaler.transform(X_test)
        
        # Make predictions
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='binary')
        recall = recall_score(y_test, y_pred, average='binary')
        f1 = f1_score(y_test, y_pred, average='binary')
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        print(f"\n{'='*60}")
        print(f"ðŸŽ¯ Model Performance Metrics:")
        print(f"{'='*60}")
        print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
        print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
        print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
        print(f"ROC-AUC:   {roc_auc:.4f}")
        print(f"{'='*60}")
        
        print("\nðŸ“‹ Detailed Classification Report:")
        print(classification_report(y_test, y_pred, target_names=['Not Similar', 'Similar']))
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        print("\nðŸ”² Confusion Matrix:")
        print(f"{'':>20} Predicted Not Similar  Predicted Similar")
        print(f"Actually Not Similar:    {cm[0][0]:<10}          {cm[0][1]:<10}")
        print(f"Actually Similar:        {cm[1][0]:<10}          {cm[1][1]:<10}")
        
        # Feature importance (for tree-based models)
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = dict(zip(self.feature_names, self.model.feature_importances_))
            print("\nðŸ” Top 10 Most Important Features:")
            for i, (feature, importance) in enumerate(
                sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10], 1
            ):
                print(f"   {i}. {feature}: {importance:.4f}")
        elif self.model_type == 'ensemble':
            # For ensemble, get feature importance from random forest
            rf_estimator = self.model.named_estimators_['rf']
            if hasattr(rf_estimator, 'feature_importances_'):
                feature_importance = dict(zip(self.feature_names, rf_estimator.feature_importances_))
                print("\nðŸ” Top 10 Most Important Features (from Random Forest):")
                for i, (feature, importance) in enumerate(
                    sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10], 1
                ):
                    print(f"   {i}. {feature}: {importance:.4f}")
        
        return {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'roc_auc': float(roc_auc),
            'confusion_matrix': cm.tolist()
        }
    
    def cross_validate(self, X, y):
        """Perform cross-validation"""
        print("\nðŸ”„ Performing 5-fold cross-validation...")
        
        X_scaled = self.scaler.fit_transform(X)
        
        cv_scores = cross_val_score(
            self.model, X_scaled, y,
            cv=5, scoring='f1', n_jobs=-1
        )
        
        print(f"Cross-validation F1 scores: {cv_scores}")
        print(f"Mean F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        return cv_scores
    
    def save_model(self, output_dir='ml_models', metrics=None):
        """Save trained model"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_filename = f"{self.model_type}_model_{timestamp}.pkl"
        scaler_filename = f"{self.model_type}_scaler_{timestamp}.pkl"
        
        model_path = os.path.join(output_dir, model_filename)
        scaler_path = os.path.join(output_dir, scaler_filename)
        
        # Save model
        joblib.dump(self.model, model_path)
        print(f"\nðŸ’¾ Model saved to: {model_path}")
        
        # Save scaler
        joblib.dump(self.scaler, scaler_path)
        print(f"ðŸ’¾ Scaler saved to: {scaler_path}")
        
        # Save as default model
        default_model_path = os.path.join(output_dir, 'default_model.pkl')
        default_scaler_path = os.path.join(output_dir, 'default_scaler.pkl')
        
        joblib.dump(self.model, default_model_path)
        joblib.dump(self.scaler, default_scaler_path)
        print(f"ðŸ’¾ Default model updated: {default_model_path}")
        
        # Save metadata
        if metrics:
            metadata = {
                'model_type': self.model_type,
                'timestamp': timestamp,
                'metrics': metrics,
                'feature_names': self.feature_names,
                'model_path': model_path,
                'scaler_path': scaler_path
            }
            
            metadata_path = model_path.replace('.pkl', '_metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            print(f"ðŸ’¾ Metadata saved to: {metadata_path}")
        
        return model_path, scaler_path
    
    def run_full_training(self):
        """Complete training pipeline"""
        print("="*60)
        print("ðŸš€ Code Similarity ML Model Training")
        print("="*60)
        
        start_time = datetime.now()
        
        # Load data
        X, y, df = self.load_data()
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\nðŸ“Š Dataset Split:")
        print(f"   Training set: {len(X_train)} samples")
        print(f"   Test set: {len(X_test)} samples")
        print(f"   Similar in training: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.2f}%)")
        print(f"   Similar in test: {sum(y_test)} ({sum(y_test)/len(y_test)*100:.2f}%)")
        
        # Create and train model
        self.create_model()
        self.train(X_train, y_train)
        
        # Evaluate model
        metrics = self.evaluate(X_test, y_test)
        
        # Cross-validation
        cv_scores = self.cross_validate(X, y)
        metrics['cv_scores'] = cv_scores.tolist()
        metrics['cv_mean'] = float(cv_scores.mean())
        
        # Save model
        model_path, scaler_path = self.save_model(metrics=metrics)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print("\n" + "="*60)
        print("âœ… Training completed successfully!")
        print("="*60)
        print(f"â±ï¸  Total time: {duration:.2f} seconds ({duration/60:.2f} minutes)")
        print(f"ðŸŽ¯ Final F1 Score: {metrics['f1_score']*100:.2f}%")
        print(f"ðŸŽ¯ Final Accuracy: {metrics['accuracy']*100:.2f}%")
        print("="*60)
        
        return metrics


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Train Code Similarity Model')
    parser.add_argument('--dataset', type=str, required=True,
                       help='Path to training dataset CSV')
    parser.add_argument('--model-type', type=str, default='ensemble',
                       choices=['random_forest', 'gradient_boosting', 'svm', 'neural_network', 'ensemble'],
                       help='Type of ML model to train')
    parser.add_argument('--output-dir', type=str, default='ml_models',
                       help='Directory to save trained model')
    
    args = parser.parse_args()
    
    # Create trainer
    trainer = AdvancedModelTrainer(
        dataset_path=args.dataset,
        model_type=args.model_type
    )
    
    # Run training
    try:
        metrics = trainer.run_full_training()
        print(f"\nðŸŽ‰ Model is ready for deployment!")
        print(f"   Use it in your Django app for code similarity detection")
        return 0
    except Exception as e:
        print(f"\nâŒ Error during training: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())

